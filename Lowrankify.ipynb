{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with low-rank model compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron clustering using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "def compute_centroids(weights, assignment):\n",
    "    # we are going to mean the neurons into the first index in the weights occuring in the assingment\n",
    "    first_indices = []\n",
    "    for i in range(int(assignment.max()) + 1):\n",
    "        indices = (assignment == i).nonzero()\n",
    "\n",
    "        first_index = indices[0]\n",
    "        \n",
    "        try:\n",
    "            first_indices.append(first_index.item())\n",
    "            weights[first_index, :] = weights[indices].mean(0)\n",
    "        except:\n",
    "            first_indices.append(first_index[0].item())\n",
    "            weights[first_index[0], :] = weights[indices[0]].mean(0)\n",
    "    first_indices.sort()\n",
    "\n",
    "    return weights[first_indices]\n",
    "\n",
    "\n",
    "def reduce_neurons(weight, bias=None, clusters=None, threshold=0.1):\n",
    "    # function that does the neuron clustering - returns new weights and biases of reduced neurons layer\n",
    "    if bias is None:\n",
    "        bias = torch.zeros((weight.shape[0]))\n",
    "\n",
    "    \n",
    "    weight = torch.concat((weight, bias.unsqueeze(-1)), 1)\n",
    "\n",
    "    normed = torch.nn.functional.normalize(weight)\n",
    "\n",
    "    D = (1.0 - (normed @ normed.T)).relu()\n",
    "\n",
    "    C = AgglomerativeClustering(clusters, affinity='precomputed', linkage='complete', compute_full_tree=True, distance_threshold=threshold)\n",
    "    assignment = C.fit_predict(D)\n",
    "\n",
    "    centroids = compute_centroids(weight, assignment)\n",
    "\n",
    "    bias, centroids = centroids[:, -1].squeeze(), centroids[:, :-1]\n",
    "\n",
    "    return centroids, bias, assignment\n",
    "\n",
    "\n",
    "def reduce_columns(weight, assignment):\n",
    "    # function that compensates for neurons that were clustered in the previous layer by aggregating the input features\n",
    "    # we are going to sum the columns into the first index in the weights occuring in the assignment\n",
    "    first_indices = []\n",
    "    for i in range(int(assignment.max())+1):\n",
    "        indices = (assignment == i).nonzero()\n",
    "\n",
    "        first_index = indices[0]\n",
    "\n",
    "        try:\n",
    "            first_indices.append(first_index.item())\n",
    "            weight[:, first_index] = weight[:, indices].sum(1)\n",
    "        except:\n",
    "            first_indices.append(first_index[0].item())\n",
    "            weight[:, first_index[0]] = weight[:, indices[0]].sum(1)\n",
    "\n",
    "    first_indices.sort()\n",
    "\n",
    "    return weight[:, first_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boring evaluation code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (X, y) in tqdm(enumerate(loader)):\n",
    "        out = model(X.to(0))\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(out, y.to(0))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    output = output.to(torch.device('cpu'))\n",
    "    target = target.to(torch.device('cpu'))\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.shape[0]\n",
    "\n",
    "    _, idx = output.sort(dim=1, descending=True)\n",
    "    pred = idx.narrow(1, 0, maxk).t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def epoch_accuracy(loader_s, student):\n",
    "    student.eval()\n",
    "\n",
    "    out_epoch_s = [accuracy(student(L.to(0)), y)[0].detach().cpu().item() for L, y in loader_s]\n",
    "\n",
    "    student.train()\n",
    "\n",
    "    return sum(out_epoch_s) / len(out_epoch_s)\n",
    "\n",
    "def test(network, test_loader):\n",
    "    network.eval().to(0)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_losses=[]\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data.to(0))\n",
    "            test_loss += torch.nn.CrossEntropyLoss()(output, target.to(0)).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1].cpu()\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def latency(f, x, trials = 100):\n",
    "    total = 0.0\n",
    "    for trial in range(trials):\n",
    "        start = time.perf_counter()\n",
    "        f(x)\n",
    "        total += time.perf_counter() - start\n",
    "    return total / trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boring training code with data and network definition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR100('./cifar100/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR100('./cifar100/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ])),\n",
    "  batch_size=1024, shuffle=True)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(3072, 2048)\n",
    "        self.fc2 = torch.nn.Linear(2048, 1024)\n",
    "        self.fc3 = torch.nn.Linear(1024, 512)\n",
    "        self.fc4 = torch.nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        return torch.nn.functional.relu(self.fc4(torch.nn.functional.relu(self.fc3(torch.nn.functional.relu(self.fc2(torch.nn.functional.relu(self.fc1(x))))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD low-rank compression layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LowRankLinear(torch.nn.Module):\n",
    "    # takes in a linear layer and decomposes it into two low-rank linear layers\n",
    "    def __init__(self, fc, rank):\n",
    "        super(LowRankLinear, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(fc.weight.shape[1], rank, bias = False)\n",
    "        self.fc2 = torch.nn.Linear(rank, fc.weight.shape[0])\n",
    "        \n",
    "        weight1 = fc.weight\n",
    "\n",
    "        self.fc2.bias = fc.bias\n",
    "\n",
    "        W1 = weight1.cpu().detach().clone().numpy()\n",
    "\n",
    "        U1, E1, V1 = np.linalg.svd(W1, False)\n",
    "\n",
    "        rd1 = np.zeros((len(E1), len(E1)))\n",
    "\n",
    "        for i, v in enumerate(E1):\n",
    "            rd1[i, i] = v\n",
    "\n",
    "\n",
    "        if fc.weight.shape[1] > fc.weight.shape[0]:\n",
    "            # if the input dom of the fc is bigger than the output dim\n",
    "            self.fc1.weight = torch.nn.parameter.Parameter(torch.tensor(rd1[:rank, :rank] @ V1[:rank, :]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "            self.fc2.weight = torch.nn.parameter.Parameter(torch.tensor(U1[:, :rank]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "        else:\n",
    "            self.fc1.weight = torch.nn.parameter.Parameter(torch.tensor(V1[:rank, :]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "            self.fc2.weight = torch.nn.parameter.Parameter(torch.tensor(U1[:, :rank] @ rd1[:rank, :rank]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.fc1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boring model training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:17, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 478/10000 (5%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:11, 33.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 765/10000 (8%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:11, 33.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0044, Accuracy: 1011/10000 (10%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:11, 33.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1082/10000 (11%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:11, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1122/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "network = Net().to(0)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=1e-2,\n",
    "                      momentum=0.5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(network.to(0), optimizer, train_loader)\n",
    "    gc.collect()\n",
    "    test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the compressed models' latency (cpu) and accuracy\n",
    "latency doesn't change so much on the gpu as it is not FLOPS-bound with such a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005990189999931772\n",
      "\n",
      "Test set: Avg. loss: 0.0046, Accuracy: 319/10000 (3%)\n",
      "\n",
      "0.0006293620000008104\n",
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 594/10000 (6%)\n",
      "\n",
      "0.000699129000005314\n",
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 631/10000 (6%)\n",
      "\n",
      "0.0007874779999883686\n",
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 694/10000 (7%)\n",
      "\n",
      "0.0008414289999984703\n",
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 815/10000 (8%)\n",
      "\n",
      "0.001249190999996017\n",
      "\n",
      "Test set: Avg. loss: 0.0044, Accuracy: 1028/10000 (10%)\n",
      "\n",
      "0.005720809999996845\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1016/10000 (10%)\n",
      "\n",
      "0.005700563999998849\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1033/10000 (10%)\n",
      "\n",
      "0.005686595999995916\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1040/10000 (10%)\n",
      "\n",
      "0.005887784000002512\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1037/10000 (10%)\n",
      "\n",
      "0.005946083000003455\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1049/10000 (10%)\n",
      "\n",
      "0.005767096000006404\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1046/10000 (10%)\n",
      "\n",
      "0.006664970000003905\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1031/10000 (10%)\n",
      "\n",
      "0.006209335999999439\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1044/10000 (10%)\n",
      "\n",
      "0.006238947000000507\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1023/10000 (10%)\n",
      "\n",
      "0.005994363000000931\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1008/10000 (10%)\n",
      "\n",
      "0.00611313999999652\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1067/10000 (11%)\n",
      "\n",
      "0.006121806999999535\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1040/10000 (10%)\n",
      "\n",
      "0.005980310999992753\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1025/10000 (10%)\n",
      "\n",
      "0.005958710999994992\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1023/10000 (10%)\n",
      "\n",
      "0.0061532800000020415\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1020/10000 (10%)\n",
      "\n",
      "0.006133078999998816\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1040/10000 (10%)\n",
      "\n",
      "0.0061103630000047815\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1022/10000 (10%)\n",
      "\n",
      "0.00604406100000233\n",
      "\n",
      "Test set: Avg. loss: 0.0043, Accuracy: 1024/10000 (10%)\n",
      "\n",
      "0.005880522999999585\n",
      "\n",
      "Test set: Avg. loss: 0.0044, Accuracy: 931/10000 (9%)\n",
      "\n",
      "0.003193304000004673\n",
      "\n",
      "Test set: Avg. loss: 0.0045, Accuracy: 624/10000 (6%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.cpu()\n",
    "compressed_net = copy(network)\n",
    "\n",
    "def lowranklatency(module, rank):\n",
    "    module = copy(module)\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            module.add_module(name, LowRankLinear(mod, rank))\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(64, 3, 32, 32)))\n",
    "    test(module, test_loader)\n",
    "\n",
    "def clusteredneuronlatency(module, threshold):\n",
    "    module = copy(module)\n",
    "    for (name, mod), (name_next, mod_next) in zip(module.named_modules(), list(iter(module.named_modules()))[1:]):\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            weights, bias, assignment = reduce_neurons(mod.weight.detach().clone(), mod.bias.detach().clone(), threshold=threshold)\n",
    "            cols = reduce_columns(mod_next.weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            mod_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            mod_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            mod_next.bias = mod_next.bias\n",
    "\n",
    "            module.add_module(name, mod)\n",
    "            module.add_module(name_next, mod_next)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(64, 3, 32, 32)))\n",
    "    test(module, test_loader)\n",
    "\n",
    "\n",
    "def clusteredlowrank(module, rank, threshold):\n",
    "    module = copy(module)\n",
    "\n",
    "    for (name, mod), (name_next, mod_next) in zip(module.named_modules(), list(iter(module.named_modules()))[1:]):\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            weights, bias, assignment = reduce_neurons(mod.weight.detach().clone(), mod.bias.detach().clone(), threshold=threshold)\n",
    "            cols = reduce_columns(mod_next.weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            mod_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            mod_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            mod_next.bias = mod_next.bias\n",
    "\n",
    "            module.add_module(name, mod)\n",
    "            module.add_module(name_next, mod_next)\n",
    "\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            module.add_module(name, LowRankLinear(mod, rank))\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(64, 3, 32, 32)))\n",
    "    test(module, test_loader)\n",
    "\n",
    "for p in range(2, 8):\n",
    "    lowranklatency(compressed_net, 2**p)\n",
    "\n",
    "for d in range(0, 20, 1):\n",
    "    clusteredneuronlatency(compressed_net, d / 20)\n",
    "\n",
    "clusteredlowrank(compressed_net, 2, 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
