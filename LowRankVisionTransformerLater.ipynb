{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with low-rank model compression\n",
    "## this time with vision transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaycr\\anaconda3\\envs\\tfnew\\lib\\site-packages\\torch\\hub.py:365: UserWarning: TORCH_HUB is deprecated, please use env TORCH_HOME instead\n",
      "  warnings.warn('TORCH_HUB is deprecated, please use env TORCH_HOME instead')\n",
      "Using cache found in ./hub\\facebookresearch_dinov2_main\n",
      "c:\\Users\\jaycr\\Desktop\\pytorch repositories\\Low-Rank-Model-Compression\\./hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "c:\\Users\\jaycr\\Desktop\\pytorch repositories\\Low-Rank-Model-Compression\\./hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "c:\\Users\\jaycr\\Desktop\\pytorch repositories\\Low-Rank-Model-Compression\\./hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import gc\n",
    "\n",
    "from copy import deepcopy as copy\n",
    "import time\n",
    "\n",
    "\n",
    "def dino_model():\n",
    "    os.environ['TORCH_HOME'] = './'\n",
    "    os.environ['TORCH_HUB'] = './'\n",
    "    # DINOv2 vit-s (14) with registers\n",
    "    model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "    # state = model.state_dict()\n",
    "    # mymodel = vit_small(14, 4)\n",
    "    # mymodel.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    return model.to('cpu')\n",
    "\n",
    "def dino_transforms():\n",
    "    return v2.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        transforms.Resize(size=(256, 256), antialias=True),\n",
    "                        transforms.CenterCrop((224, 224)),\n",
    "                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225]\n",
    "                                            ),\n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "DINOv2 = dino_model()\n",
    "DINOv2_transform = dino_transforms()\n",
    "\n",
    "def compute_centroids(weights, assignment):\n",
    "    # we are going to mean the neurons into the first index in the weights occuring in the assingment\n",
    "    first_indices = []\n",
    "    for i in range(int(assignment.max()) + 1):\n",
    "        indices = (assignment == i).nonzero()\n",
    "\n",
    "        first_index = indices[0]\n",
    "        \n",
    "        try:\n",
    "            first_indices.append(first_index.item())\n",
    "            weights[first_index, :] = weights[indices].mean(0)\n",
    "        except:\n",
    "            first_indices.append(first_index[0].item())\n",
    "            weights[first_index[0], :] = weights[indices[0]].mean(0)\n",
    "    first_indices.sort()\n",
    "\n",
    "    return weights[first_indices]\n",
    "\n",
    "\n",
    "def reduce_neurons(weight, bias=None, clusters=None, threshold=0.1):\n",
    "    # function that does the neuron clustering - returns new weights and biases of reduced neurons layer\n",
    "    if bias is None:\n",
    "        bias = torch.zeros((weight.shape[0]))\n",
    "\n",
    "    \n",
    "    weight = torch.concat((weight, bias.unsqueeze(-1)), 1)\n",
    "\n",
    "    normed = torch.nn.functional.normalize(weight)\n",
    "\n",
    "    D = (1.0 - (normed @ normed.T)).relu()\n",
    "\n",
    "    C = AgglomerativeClustering(clusters, affinity='precomputed', linkage='complete', compute_full_tree=True, distance_threshold=threshold)\n",
    "    assignment = C.fit_predict(D)\n",
    "\n",
    "    centroids = compute_centroids(weight, assignment)\n",
    "\n",
    "    bias, centroids = centroids[:, -1].squeeze(), centroids[:, :-1]\n",
    "\n",
    "    return centroids, bias, assignment\n",
    "\n",
    "\n",
    "def reduce_columns(weight, assignment):\n",
    "    # function that compensates for neurons that were clustered in the previous layer by aggregating the input features\n",
    "    # we are going to sum the columns into the first index in the weights occuring in the assignment\n",
    "    first_indices = []\n",
    "    for i in range(int(assignment.max())+1):\n",
    "        indices = (assignment == i).nonzero()\n",
    "\n",
    "        first_index = indices[0]\n",
    "\n",
    "        try:\n",
    "            first_indices.append(first_index.item())\n",
    "            weight[:, first_index] = weight[:, indices].sum(1)\n",
    "        except:\n",
    "            first_indices.append(first_index[0].item())\n",
    "            weight[:, first_index[0]] = weight[:, indices[0]].sum(1)\n",
    "\n",
    "    first_indices.sort()\n",
    "\n",
    "    return weight[:, first_indices]\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (X, y) in tqdm(enumerate(loader)):\n",
    "        out = model(X.to(0))\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(out, y.to(0))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    output = output.to(torch.device('cpu'))\n",
    "    target = target.to(torch.device('cpu'))\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.shape[0]\n",
    "\n",
    "    _, idx = output.sort(dim=1, descending=True)\n",
    "    pred = idx.narrow(1, 0, maxk).t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def epoch_accuracy(loader_s, student):\n",
    "    student.eval()\n",
    "\n",
    "    out_epoch_s = [accuracy(student(L.to(0)), y)[0].detach().cpu().item() for L, y in loader_s]\n",
    "\n",
    "    student.train()\n",
    "\n",
    "    return sum(out_epoch_s) / len(out_epoch_s)\n",
    "\n",
    "def test(network, test_loader):\n",
    "    network.eval().to(0)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_losses=[]\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data.to(0))\n",
    "            test_loss += torch.nn.CrossEntropyLoss()(output, target.to(0)).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1].cpu()\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def latency(f, x, trials = 100):\n",
    "    f.cpu()\n",
    "    total = 0.0\n",
    "    for trial in range(trials):\n",
    "        start = time.perf_counter()\n",
    "        f(x)\n",
    "        total += time.perf_counter() - start\n",
    "    return total / trials\n",
    "\n",
    "\n",
    "class LowRankLinear(torch.nn.Module):\n",
    "    # takes in a linear layer and decomposes it into two low-rank linear layers\n",
    "    def __init__(self, fc, rank):\n",
    "        super(LowRankLinear, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(fc.weight.shape[1], rank, bias = False)\n",
    "        self.fc2 = torch.nn.Linear(rank, fc.weight.shape[0])\n",
    "        \n",
    "        weight1 = fc.weight\n",
    "\n",
    "        self.fc2.bias = fc.bias\n",
    "\n",
    "        W1 = weight1.cpu().detach().clone().numpy()\n",
    "\n",
    "        U1, E1, V1 = np.linalg.svd(W1, False)\n",
    "\n",
    "        rd1 = np.zeros((len(E1), len(E1)))\n",
    "\n",
    "        for i, v in enumerate(E1):\n",
    "            rd1[i, i] = v\n",
    "\n",
    "\n",
    "        if fc.weight.shape[1] > fc.weight.shape[0]:\n",
    "            # if the input dom of the fc is bigger than the output dim\n",
    "            self.fc1.weight = torch.nn.parameter.Parameter(torch.tensor(rd1[:rank, :rank] @ V1[:rank, :]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "            self.fc2.weight = torch.nn.parameter.Parameter(torch.tensor(U1[:, :rank]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "        else:\n",
    "            self.fc1.weight = torch.nn.parameter.Parameter(torch.tensor(V1[:rank, :]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "            self.fc2.weight = torch.nn.parameter.Parameter(torch.tensor(U1[:, :rank] @ rd1[:rank, :rank]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.fc1(x))\n",
    "    \n",
    "def linearleaves(module):\n",
    "    # returns a list of pairs of (parent, submodule_name) pairs for all submodule leaves of the current module\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        return [(module, None)]\n",
    "\n",
    "    linear_children = []\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            linear_children.append((name, module))\n",
    "    return linear_children\n",
    "        \n",
    "\n",
    "def getattrrecur(mod, s):\n",
    "    s = s.split('.')\n",
    "    for substr in s:\n",
    "        mod = getattr(mod, substr)\n",
    "    return mod\n",
    "\n",
    "\n",
    "def setattrrecur(mod, s, value):\n",
    "    s = s.split('.')\n",
    "    for substr in s[:-1]:\n",
    "        mod = getattr(mod, substr)\n",
    "    setattr(mod, s[-1], value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will use this custom module to assess the performance of the embedding model on a simple task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.m = module\n",
    "        self.linear = torch.nn.Linear(768, 102)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.m(x).detach()\n",
    "        return self.linear(x) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./hub\\facebookresearch_dinov2_main\n",
      "64it [00:34,  1.83it/s]\n",
      "64it [00:25,  2.55it/s]\n",
      "64it [00:25,  2.54it/s]\n",
      "64it [00:25,  2.52it/s]\n",
      "64it [00:25,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0176, Accuracy: 1011/1020 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = torchvision.datasets.Flowers102('./Flowers102', split='train', transform=DINOv2_transform, download=True)\n",
    "val_ds = torchvision.datasets.Flowers102('./Flowers102', split='val', transform=DINOv2_transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=48)\n",
    "\n",
    "DINOv2 = dino_model()\n",
    "\n",
    "model = LinearProbe(DINOv2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(model.to(0), optimizer, train_loader)\n",
    "    gc.collect()\n",
    "test(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering the neurons only occurs in the fc layers of the transformer for now (these are responsible for the majority of the parameters)\n",
    "## rank reduction only occurs when it would reduce model latency (when the rank is reduced by more than half)\n",
    "\n",
    "see the previous notebook for more details on these two approaches\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/2206.06072.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteredlowrank(module, rank, threshold):\n",
    "    module = copy(module)\n",
    "\n",
    "    for (name, mod), (name_next, mod_next) in zip(linearleaves(module), linearleaves(module)[1:]):\n",
    "        if 'fc1' in name:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "\n",
    "    for name, mod in linearleaves(module):\n",
    "        setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "    test(module, val_loader)\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "def lowranklatency(module, rank):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "        layers_reduced += 1\n",
    "\n",
    "    print(f'layers reduced: {layers_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def laterlowranklatency(module, rank, after=27):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        if i > after:\n",
    "            setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "            layers_reduced += 1\n",
    "\n",
    "    print(f'layers reduced: {layers_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def earlierlowranklatency(module, rank, before=8):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        if i < before:\n",
    "            setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "            layers_reduced += 1\n",
    "\n",
    "    print(f'layers reduced: {layers_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def clusteredlatency(module, threshold):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for (name, mod), (name_next, mod_next) in zip(linearleaves(module), linearleaves(module)[1:]):\n",
    "        if 'fc1' in name:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    print(f'neurons reduced: {neurons_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def laterclusteredlatency(module, threshold, after=28):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        if 'fc1' in name and i > after:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    print(f'neurons reduced: {neurons_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def earlierclusteredlatency(module, threshold, before=28):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        if 'fc1' in name and i < before:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    print(f'neurons reduced: {neurons_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.258001792999998\n",
      "\n",
      "Test set: Avg. loss: 0.0176, Accuracy: 1011/1020 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "print(latency(model.eval(), torch.ones(1, 3, 224, 224)))\n",
    "test(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bfloats are actually much (~100x) slower on the cpu than float32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.134958201000014\n"
     ]
    }
   ],
   "source": [
    "model.bfloat16()\n",
    "print(latency(model.eval(), torch.ones(1, 3, 224, 224).bfloat16()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaycr\\AppData\\Local\\Temp/ipykernel_22476/4007515744.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  weight[:, first_index] = weight[:, indices].sum(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 3927 (12.91650128364563s)\n",
      "\n",
      "Test set: Avg. loss: 0.2549, Accuracy: 774/1020 (76%)\n",
      "\n",
      "0.24229154299999664\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "clusteredlatency(evaluating, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 49 (199.4038803577423s)\n",
      "\n",
      "Test set: Avg. loss: 2.9148, Accuracy: 11/1020 (1%)\n",
      "\n",
      "0.17873593699999787\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "lowranklatency(evaluating, 382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 1 (0.032996177673339844s)\n",
      "\n",
      "Test set: Avg. loss: 0.0176, Accuracy: 1011/1020 (99%)\n",
      "\n",
      "0.23260513299999502\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "laterlowranklatency(evaluating, 382, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 5 (17.37299418449402s)\n",
      "\n",
      "Test set: Avg. loss: 0.3358, Accuracy: 728/1020 (71%)\n",
      "\n",
      "0.24638663100000713\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "laterlowranklatency(evaluating, 382, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 9 (34.76936674118042s)\n",
      "\n",
      "Test set: Avg. loss: 0.9883, Accuracy: 387/1020 (38%)\n",
      "\n",
      "0.24058260899999367\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "laterlowranklatency(evaluating, 382, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 4 (17.93900990486145s)\n",
      "\n",
      "Test set: Avg. loss: 0.0198, Accuracy: 1007/1020 (99%)\n",
      "\n",
      "0.24273931499998752\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "earlierlowranklatency(evaluating, 382, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 8 (34.327433824539185s)\n",
      "\n",
      "Test set: Avg. loss: 0.0213, Accuracy: 1000/1020 (98%)\n",
      "\n",
      "0.2465916739999966\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "earlierlowranklatency(evaluating, 382, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 12 (57.95983099937439s)\n",
      "\n",
      "Test set: Avg. loss: 0.3101, Accuracy: 794/1020 (78%)\n",
      "\n",
      "0.2322501379999949\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "earlierlowranklatency(evaluating, 382, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaycr\\AppData\\Local\\Temp/ipykernel_2028/1720078378.py:100: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  weight[:, first_index] = weight[:, indices].sum(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 819 (0.8727188110351562s)\n",
      "\n",
      "Test set: Avg. loss: 0.0173, Accuracy: 1012/1020 (99%)\n",
      "\n",
      "0.2387627739999948\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "earlierclusteredlatency(evaluating, 0.1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 1094 (1.9740779399871826s)\n",
      "\n",
      "Test set: Avg. loss: 0.0171, Accuracy: 1011/1020 (99%)\n",
      "\n",
      "0.2560333930000161\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "earlierclusteredlatency(evaluating, 0.1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 1290 (2.8597729206085205s)\n",
      "\n",
      "Test set: Avg. loss: 0.0175, Accuracy: 1010/1020 (99%)\n",
      "\n",
      "0.24357506200003173\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "earlierclusteredlatency(evaluating, 0.1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 1103 (0.8631227016448975s)\n",
      "\n",
      "Test set: Avg. loss: 0.1698, Accuracy: 840/1020 (82%)\n",
      "\n",
      "0.26188703099998745\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "laterclusteredlatency(evaluating, 0.1, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 1709 (1.8653233051300049s)\n",
      "\n",
      "Test set: Avg. loss: 0.2302, Accuracy: 809/1020 (79%)\n",
      "\n",
      "0.2436918040000046\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "laterclusteredlatency(evaluating, 0.1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 3.3223, Accuracy: 10/1020 (1%)\n",
      "\n",
      "0.2702627630000006\n"
     ]
    }
   ],
   "source": [
    "evaluating = copy(model)\n",
    "clusteredlowrank(evaluating, rank=384, threshold=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
