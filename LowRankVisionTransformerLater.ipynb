{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with low-rank model compression\n",
    "## this time with vision transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/envs/torch/lib/python3.12/site-packages/torch/hub.py:365: UserWarning: TORCH_HUB is deprecated, please use env TORCH_HOME instead\n",
      "  warnings.warn('TORCH_HUB is deprecated, please use env TORCH_HOME instead')\n",
      "Using cache found in ./hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import gc\n",
    "\n",
    "from copy import deepcopy as copy\n",
    "import time\n",
    "\n",
    "\n",
    "def dino_model():\n",
    "    os.environ['TORCH_HOME'] = './'\n",
    "    os.environ['TORCH_HUB'] = './'\n",
    "    # DINOv2 vit-s (14) with registers\n",
    "    # model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "    model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "    # state = model.state_dict()\n",
    "    # mymodel = vit_small(14, 4)\n",
    "    # mymodel.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    return model.to('cpu')\n",
    "\n",
    "def dino_transforms():\n",
    "    return v2.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        transforms.Resize(size=(256, 256), antialias=True),\n",
    "                        transforms.CenterCrop((224, 224)),\n",
    "                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225]\n",
    "                                            ),\n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "DINOv2 = dino_model()\n",
    "DINOv2_transform = dino_transforms()\n",
    "\n",
    "def compute_centroids(weights, assignment):\n",
    "    # we are going to mean the neurons into the first index in the weights occuring in the assingment\n",
    "    first_indices = []\n",
    "    for i in range(int(assignment.max()) + 1):\n",
    "        indices = (assignment == i).nonzero()\n",
    "\n",
    "        first_index = indices[0]\n",
    "        \n",
    "        try:\n",
    "            first_indices.append(first_index.item())\n",
    "            weights[first_index, :] = weights[indices].mean(0)\n",
    "        except:\n",
    "            first_indices.append(first_index[0].item())\n",
    "            weights[first_index[0], :] = weights[indices[0]].mean(0)\n",
    "    first_indices.sort()\n",
    "\n",
    "    return weights[first_indices]\n",
    "\n",
    "\n",
    "def reduce_neurons(weight, bias=None, clusters=None, threshold=0.1):\n",
    "    # function that does the neuron clustering - returns new weights and biases of reduced neurons layer\n",
    "    if bias is None:\n",
    "        bias = torch.zeros((weight.shape[0]))\n",
    "\n",
    "    \n",
    "    weight = torch.concat((weight, bias.unsqueeze(-1)), 1)\n",
    "\n",
    "    normed = torch.nn.functional.normalize(weight)\n",
    "\n",
    "    D = (1.0 - (normed @ normed.T)).relu()\n",
    "\n",
    "    C = AgglomerativeClustering(clusters, metric='precomputed', linkage='complete', compute_full_tree=True, distance_threshold=threshold)\n",
    "    assignment = C.fit_predict(D)\n",
    "\n",
    "    centroids = compute_centroids(weight, assignment)\n",
    "\n",
    "    bias, centroids = centroids[:, -1].squeeze(), centroids[:, :-1]\n",
    "\n",
    "    return centroids, bias, assignment\n",
    "\n",
    "\n",
    "def reduce_columns(weight, assignment):\n",
    "    # function that compensates for neurons that were clustered in the previous layer by aggregating the input features\n",
    "    # we are going to sum the columns into the first index in the weights occuring in the assignment\n",
    "    first_indices = []\n",
    "    for i in range(int(assignment.max())+1):\n",
    "        indices = (assignment == i).nonzero()\n",
    "\n",
    "        first_index = indices[0]\n",
    "\n",
    "        try:\n",
    "            first_indices.append(first_index.item())\n",
    "            weight[:, first_index] = weight[:, indices].sum(1)\n",
    "        except:\n",
    "            first_indices.append(first_index[0].item())\n",
    "            weight[:, first_index[0]] = weight[:, indices[0]].sum(1)\n",
    "\n",
    "    first_indices.sort()\n",
    "\n",
    "    return weight[:, first_indices]\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (X, y) in tqdm(enumerate(loader)):\n",
    "        out = model(X.to(0))\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(out, y.to(0))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    output = output.to(torch.device('cpu'))\n",
    "    target = target.to(torch.device('cpu'))\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.shape[0]\n",
    "\n",
    "    _, idx = output.sort(dim=1, descending=True)\n",
    "    pred = idx.narrow(1, 0, maxk).t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(dim=0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def epoch_accuracy(loader_s, student):\n",
    "    student.eval()\n",
    "\n",
    "    out_epoch_s = [accuracy(student(L.to(0)), y)[0].detach().cpu().item() for L, y in loader_s]\n",
    "\n",
    "    student.train()\n",
    "\n",
    "    return sum(out_epoch_s) / len(out_epoch_s)\n",
    "\n",
    "def test(network, test_loader, dtype=torch.float32, silent=False):\n",
    "    network.eval().to(0)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_losses=[]\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data.to(0).type(dtype))\n",
    "            test_loss += torch.nn.CrossEntropyLoss()(output, target.to(0)).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1].cpu()\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        if not silent:\n",
    "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    if silent:\n",
    "        return 100. * correct / len(test_loader.dataset) \n",
    "\n",
    "\n",
    "\n",
    "def latency(f, x, trials = 100):\n",
    "    f.cpu()\n",
    "    total = 0.0\n",
    "    for trial in range(trials):\n",
    "        start = time.perf_counter()\n",
    "        f(x)\n",
    "        total += time.perf_counter() - start\n",
    "    return total / trials\n",
    "\n",
    "\n",
    "class LowRankLinear(torch.nn.Module):\n",
    "    # takes in a linear layer and decomposes it into two low-rank linear layers\n",
    "    def __init__(self, fc, rank):\n",
    "        super(LowRankLinear, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(fc.weight.shape[1], rank, bias = False)\n",
    "        self.fc2 = torch.nn.Linear(rank, fc.weight.shape[0])\n",
    "        \n",
    "        weight1 = fc.weight\n",
    "\n",
    "        self.fc2.bias = fc.bias\n",
    "\n",
    "        W1 = weight1.cpu().detach().clone().numpy()\n",
    "\n",
    "        U1, E1, V1 = np.linalg.svd(W1, False)\n",
    "\n",
    "        rd1 = np.zeros((len(E1), len(E1)))\n",
    "\n",
    "        for i, v in enumerate(E1):\n",
    "            rd1[i, i] = v\n",
    "\n",
    "\n",
    "        if fc.weight.shape[1] > fc.weight.shape[0]:\n",
    "            # if the input dom of the fc is bigger than the output dim\n",
    "            self.fc1.weight = torch.nn.parameter.Parameter(torch.tensor(rd1[:rank, :rank] @ V1[:rank, :]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "            self.fc2.weight = torch.nn.parameter.Parameter(torch.tensor(U1[:, :rank]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "        else:\n",
    "            self.fc1.weight = torch.nn.parameter.Parameter(torch.tensor(V1[:rank, :]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "            self.fc2.weight = torch.nn.parameter.Parameter(torch.tensor(U1[:, :rank] @ rd1[:rank, :rank]).to(fc.weight.device).type(fc.weight.dtype))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.fc1(x))\n",
    "    \n",
    "def linearleaves(module):\n",
    "    # returns a list of pairs of (parent, submodule_name) pairs for all submodule leaves of the current module\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        return [(module, None)]\n",
    "\n",
    "    linear_children = []\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, torch.nn.Linear):\n",
    "            linear_children.append((name, module))\n",
    "    return linear_children\n",
    "        \n",
    "\n",
    "def getattrrecur(mod, s):\n",
    "    s = s.split('.')\n",
    "    for substr in s:\n",
    "        mod = getattr(mod, substr)\n",
    "    return mod\n",
    "\n",
    "\n",
    "def setattrrecur(mod, s, value):\n",
    "    s = s.split('.')\n",
    "    for substr in s[:-1]:\n",
    "        mod = getattr(mod, substr)\n",
    "    setattr(mod, s[-1], value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will use this custom module to assess the performance of the embedding model on a simple task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.m = module\n",
    "        self.linear = torch.nn.Linear(384, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.m(x).detach()\n",
    "        return self.linear(x) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./hub/facebookresearch_dinov2_main\n",
      "391it [00:47,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0207, Accuracy: 9385/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0179, Accuracy: 9425/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0169, Accuracy: 9425/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0162, Accuracy: 9435/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0161, Accuracy: 9450/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0174, Accuracy: 9416/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0189, Accuracy: 9431/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0178, Accuracy: 9448/10000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0180, Accuracy: 9453/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0191, Accuracy: 9446/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = torchvision.datasets.CIFAR10('./cifar10', train=True, transform=DINOv2_transform, download=True)\n",
    "val_ds = torchvision.datasets.CIFAR10('./cifar10', train=False, transform=DINOv2_transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=128)\n",
    "\n",
    "DINOv2 = dino_model()\n",
    "\n",
    "model = LinearProbe(DINOv2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model.to(0), optimizer, train_loader)\n",
    "    gc.collect()\n",
    "    test(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering the neurons only occurs in the fc layers of the transformer for now (these are responsible for the majority of the parameters)\n",
    "## rank reduction only occurs when it would reduce model latency (when the rank is reduced by more than half)\n",
    "\n",
    "see the previous notebook for more details on these two approaches\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/2206.06072.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteredlowrank(module, rank, threshold):\n",
    "    module = copy(module)\n",
    "\n",
    "    for (name, mod), (name_next, mod_next) in zip(linearleaves(module), linearleaves(module)[1:]):\n",
    "        if 'fc1' in name:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "\n",
    "    for name, mod in linearleaves(module):\n",
    "        setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "    test(module, val_loader)\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "def lowranklatency(module, rank):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "        layers_reduced += 1\n",
    "\n",
    "    print(f'layers reduced: {layers_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def laterlowranklatency(module, rank, after=27):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        if i > after:\n",
    "            setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "            layers_reduced += 1\n",
    "\n",
    "    print(f'layers reduced: {layers_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def earlierlowranklatency(module, rank, before=8):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        if i < before:\n",
    "            setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "            layers_reduced += 1\n",
    "\n",
    "    print(f'layers reduced: {layers_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def clusteredlatency(module, threshold):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for (name, mod), (name_next, mod_next) in zip(linearleaves(module), linearleaves(module)[1:]):\n",
    "        if 'fc1' in name:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    print(f'neurons reduced: {neurons_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def laterclusteredlatency(module, threshold, after=28):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        if 'fc1' in name and i > after:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    print(f'neurons reduced: {neurons_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "\n",
    "\n",
    "def earlierclusteredlatency(module, threshold, before=28):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        if 'fc1' in name and i < before:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    print(f'neurons reduced: {neurons_reduced} ({time.time() - start}s)')\n",
    "    test(module, val_loader)\n",
    "\n",
    "    print(latency(module.eval(), torch.ones(1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022223653524415566\n",
      "\n",
      "Test set: Avg. loss: 0.0191, Accuracy: 9446/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "print(latency(model.eval(), torch.ones(1, 3, 224, 224)))\n",
    "test(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bfloats are actually much (~100x) slower on the cpu than float32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015075424829265103\n",
      "\n",
      "Test set: Avg. loss: 0.0190, Accuracy: 9458/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.bfloat16()\n",
    "print(latency(model.eval(), torch.ones(1, 3, 224, 224).bfloat16()))\n",
    "test(model, val_loader, torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 4268 (0.6930887699127197s)\n",
      "\n",
      "Test set: Avg. loss: 0.1799, Accuracy: 5494/10000 (55%)\n",
      "\n",
      "0.020800426495261492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:51,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0190, Accuracy: 9446/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "clusteredlatency(evaluating, 0.1)\n",
    "train(evaluating.to(0), optimizer, train_loader)\n",
    "test(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 49 (1.2263009548187256s)\n",
      "\n",
      "Test set: Avg. loss: 0.9868, Accuracy: 1078/10000 (11%)\n",
      "\n",
      "0.020608259348664434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:53,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0190, Accuracy: 9446/10000 (94%)\n",
      "\n",
      "parameters: 22061962 -> 22061962 (- 0)\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "inital_params = count_parameters(evaluating)\n",
    "lowranklatency(evaluating, 95)\n",
    "train(evaluating.to(0), optimizer, train_loader)\n",
    "test(evaluating, val_loader)\n",
    "final_params = count_parameters(evaluating)\n",
    "print(f'parameters: {inital_params} -> {final_params} (- {inital_params - final_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 49 (1.237255573272705s)\n",
      "\n",
      "Test set: Avg. loss: 0.8855, Accuracy: 991/10000 (10%)\n",
      "\n",
      "0.021984323041979222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:52,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 49 (1.186415195465088s)\n",
      "\n",
      "Test set: Avg. loss: 0.9868, Accuracy: 1078/10000 (11%)\n",
      "\n",
      "0.020019255420193074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:52,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0190, Accuracy: 9446/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "inital_params = count_parameters(evaluating)\n",
    "lowranklatency(evaluating, 190)\n",
    "train(evaluating.to(0), optimizer, train_loader)\n",
    "test(model, val_loader)\n",
    "lowranklatency(evaluating, 95)\n",
    "train(evaluating.to(0), optimizer, train_loader)\n",
    "test(model, val_loader)\n",
    "final_params = count_parameters(evaluating)\n",
    "print(f'parameters: {inital_params} -> {final_params} (- {inital_params - final_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 4 (0.07844781875610352s)\n",
      "\n",
      "Test set: Avg. loss: 0.0349, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "0.021354977586306632\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "laterlowranklatency(evaluating, 192, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 8 (0.18579506874084473s)\n",
      "\n",
      "Test set: Avg. loss: 0.1111, Accuracy: 6845/10000 (68%)\n",
      "\n",
      "0.02141873525455594\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "laterlowranklatency(evaluating, 192, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 12 (0.26773953437805176s)\n",
      "\n",
      "Test set: Avg. loss: 0.2637, Accuracy: 4445/10000 (44%)\n",
      "\n",
      "0.021434388312045485\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "laterlowranklatency(evaluating, 192, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 4 (0.10966730117797852s)\n",
      "\n",
      "Test set: Avg. loss: 0.0189, Accuracy: 9454/10000 (95%)\n",
      "\n",
      "0.021378000401891768\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "earlierlowranklatency(evaluating, 382, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 8 (0.21886730194091797s)\n",
      "\n",
      "Test set: Avg. loss: 0.0190, Accuracy: 9445/10000 (94%)\n",
      "\n",
      "0.021889947098679842\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "earlierlowranklatency(evaluating, 382, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers reduced: 12 (0.32404375076293945s)\n",
      "\n",
      "Test set: Avg. loss: 0.0190, Accuracy: 9447/10000 (94%)\n",
      "\n",
      "0.022367348104016856\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "earlierlowranklatency(evaluating, 382, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 475 (0.052613258361816406s)\n",
      "\n",
      "Test set: Avg. loss: 0.0236, Accuracy: 9358/10000 (94%)\n",
      "\n",
      "0.021404069961281493\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "earlierclusteredlatency(evaluating, 0.1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 661 (0.11128783226013184s)\n",
      "\n",
      "Test set: Avg. loss: 0.0241, Accuracy: 9330/10000 (93%)\n",
      "\n",
      "0.02182576248771511\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "earlierclusteredlatency(evaluating, 0.1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 798 (0.17936372756958008s)\n",
      "\n",
      "Test set: Avg. loss: 0.0280, Accuracy: 9253/10000 (93%)\n",
      "\n",
      "0.021892040913226083\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "earlierclusteredlatency(evaluating, 0.1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 638 (0.05638718605041504s)\n",
      "\n",
      "Test set: Avg. loss: 0.0202, Accuracy: 9392/10000 (94%)\n",
      "\n",
      "0.022875919700600208\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "laterclusteredlatency(evaluating, 0.1, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons reduced: 1666 (0.08997321128845215s)\n",
      "\n",
      "Test set: Avg. loss: 0.0528, Accuracy: 8298/10000 (83%)\n",
      "\n",
      "0.02122095245984383\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model)\n",
    "laterclusteredlatency(evaluating, 0.1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0201, Accuracy: 9425/10000 (94%)\n",
      "\n",
      "0.02682997809140943\n"
     ]
    }
   ],
   "source": [
    "evaluating = copy(model)\n",
    "clusteredlowrank(evaluating, rank=384, threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our hypothesis by lattice descent on the compression graph and then a more exhaustive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdlayer(module, rank, name=None):\n",
    "    module = copy(module).cpu()\n",
    "    start = time.time()\n",
    "    \n",
    "    setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def svdmodel(module, rank):\n",
    "    module = copy(module).cpu()\n",
    "    layers_reduced = 0\n",
    "    for i, (name, mod) in enumerate(linearleaves(module)):\n",
    "        setattrrecur(module, name, LowRankLinear(getattrrecur(module, name), rank))\n",
    "        layers_reduced += 1\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def clusterlayer(module, threshold, layer=0):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        if 'fc1' in name and i == layer:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    return module, neurons_reduced\n",
    "\n",
    "\n",
    "\n",
    "def clustermodel(module, threshold):\n",
    "    module = copy(module)\n",
    "    start = time.time()\n",
    "\n",
    "    neurons_reduced = 0\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        if 'fc1' in name:\n",
    "            weights, bias, assignment = reduce_neurons(getattrrecur(mod, name).weight.detach().clone(), getattrrecur(mod, name).bias.detach().clone(), threshold=threshold)\n",
    "\n",
    "            neurons_reduced += (torch.tensor(getattrrecur(mod, name).weight.shape) - torch.tensor(weights.shape)).sum()\n",
    "\n",
    "            cols = reduce_columns(getattrrecur(mod_next, name_next).weight.detach().clone(), assignment)\n",
    "\n",
    "            mod = torch.nn.Linear(weights.shape[1], weights.shape[0])\n",
    "            mod.weight = torch.nn.Parameter(weights.detach().clone())\n",
    "            mod.bias = torch.nn.Parameter(bias.detach().clone())\n",
    "\n",
    "            replacement_next = torch.nn.Linear(cols.shape[1], cols.shape[0])\n",
    "            replacement_next.weight = torch.nn.Parameter(cols.detach().clone())\n",
    "            replacement_next.bias = getattrrecur(mod_next, name_next).bias\n",
    "\n",
    "            setattrrecur(module, name, mod)\n",
    "            setattrrecur(module, name_next, replacement_next)\n",
    "\n",
    "    return module, neurons_reduced\n",
    "\n",
    "\n",
    "def getclusterablelayers(module):\n",
    "    cluster_idxs = []\n",
    "    lowrank_idxs = []\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        lowrank_idxs.append(i)\n",
    "        if 'fc1' in name:\n",
    "            cluster_idxs.append(i)\n",
    "\n",
    "    return cluster_idxs\n",
    "\n",
    "def latticedescenteval(module, tolerance=4):\n",
    "    cluster_idxs = getclusterablelayers(module)\n",
    "    module = copy(module)\n",
    "    inital_acc = test(module, val_loader, silent=True)\n",
    "    initial_params = count_parameters(module)\n",
    "\n",
    "    lattice = [(module, inital_acc)]\n",
    "\n",
    "    reduced = []\n",
    "    neurons_reduced = []\n",
    "    \n",
    "    while max(lattice, key=lambda k: k[-1])[-1] > inital_acc - tolerance:\n",
    "        module = lattice[0][0]\n",
    "        print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "        lattice.clear()\n",
    "        for i in cluster_idxs:\n",
    "            m, neurons = clusterlayer(module, 0.1, layer=i)\n",
    "            distance = 0.1\n",
    "            while neurons < 50:\n",
    "                distance += 0.1\n",
    "                m, neurons = clusterlayer(module, distance, layer=i)\n",
    "            else:\n",
    "                acc = test(m.bfloat16(), val_loader, torch.bfloat16, silent=True)\n",
    "                lattice.append((m.float(), i, neurons, acc))\n",
    "        \n",
    "        if lattice:\n",
    "            m, i, neurons, acc = max(lattice, key=lambda k: k[-1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        train(m.to(0), optimizer, train_loader)\n",
    "\n",
    "        reduced.append(i)\n",
    "        neurons_reduced.append(neurons)\n",
    "\n",
    "        lattice = [(m, acc)]\n",
    "\n",
    "        current_params = count_parameters(m)\n",
    "        print(f'Current accuracy: {float(lattice[0][1])}% ({float(lattice[0][1]) - float(inital_acc)}), layers reduced: {reduced}; neurons reduced: {sum(neurons_reduced)} (- {initial_params - current_params} parameters)')\n",
    "    \n",
    "    return lattice[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02326177352690138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:50,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 94.91999816894531% (0.45999908447265625), layers reduced: [26]; neurons reduced: 142 (- 109198 parameters)\n",
      "0.023186166152590885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:50,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 94.75% (0.29000091552734375), layers reduced: [26, 18]; neurons reduced: 279 (- 214551 parameters)\n",
      "0.023874963453272356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:50,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 94.55000305175781% (0.09000396728515625), layers reduced: [26, 18, 10]; neurons reduced: 416 (- 319904 parameters)\n",
      "0.026018363183829933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 93.87000274658203% (-0.589996337890625), layers reduced: [26, 18, 10, 46]; neurons reduced: 1054 (- 810526 parameters)\n",
      "0.02249063953291625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:47,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 93.83000183105469% (-0.6299972534179688), layers reduced: [26, 18, 10, 46, 46]; neurons reduced: 1159 (- 891271 parameters)\n",
      "0.021818426897516473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:47,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 93.52999877929688% (-0.9300003051757812), layers reduced: [26, 18, 10, 46, 46, 6]; neurons reduced: 1345 (- 1034305 parameters)\n",
      "0.023826734247850253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 92.5199966430664% (-1.94000244140625), layers reduced: [26, 18, 10, 46, 46, 6, 22]; neurons reduced: 1456 (- 1119664 parameters)\n",
      "0.022900281874462962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 91.81999969482422% (-2.6399993896484375), layers reduced: [26, 18, 10, 46, 46, 6, 22, 22]; neurons reduced: 1507 (- 1158883 parameters)\n",
      "0.022624140278203413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 91.06999969482422% (-3.3899993896484375), layers reduced: [26, 18, 10, 46, 46, 6, 22, 22, 46]; neurons reduced: 2057 (- 1581833 parameters)\n",
      "0.020954165830044075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:48,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy: 89.16000366210938% (-5.299995422363281), layers reduced: [26, 18, 10, 46, 46, 6, 22, 22, 46, 26]; neurons reduced: 2118 (- 1628742 parameters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "module = latticedescenteval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcompressablelayers(module):\n",
    "    lowrank_names = []\n",
    "\n",
    "    for i, ((name, mod), (name_next, mod_next)) in enumerate(zip(linearleaves(module), linearleaves(module)[1:])):\n",
    "        lowrank_names.append(name)\n",
    "\n",
    "    return lowrank_names\n",
    "\n",
    "\n",
    "def lowranklatticedescenteval(module, tolerance=4):\n",
    "    lowrank_names = getcompressablelayers(module)\n",
    "    module = copy(module)\n",
    "    inital_acc = test(module, val_loader, silent=True)\n",
    "    initial_params = count_parameters(module)\n",
    "\n",
    "    lattice = [(module, inital_acc)]\n",
    "\n",
    "    reduced = []\n",
    "    \n",
    "    while max(lattice, key=lambda k: k[-1])[-1] > inital_acc - tolerance:\n",
    "        lowrank_names = getcompressablelayers(module)\n",
    "        module = lattice[0][0]\n",
    "        print(latency(module.eval(), torch.ones(1, 3, 224, 224)))\n",
    "        lattice.clear()\n",
    "\n",
    "        for name in lowrank_names:\n",
    "            \n",
    "            try:\n",
    "                min_weight = torch.tensor(getattrrecur(module, name).weight.shape).min()\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            m = svdlayer(module, min_weight // 4, name=name)\n",
    "            acc = test(m.bfloat16(), val_loader, torch.bfloat16, silent=True)\n",
    "            lattice.append((m, name, acc))\n",
    "        \n",
    "        if lattice:\n",
    "            m, i, acc = max(lattice, key=lambda k: k[-1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        train(m.float().to(0), optimizer, train_loader)\n",
    "\n",
    "        reduced.append(i)\n",
    "\n",
    "        lattice = [(m, acc)]\n",
    "\n",
    "        current_params = count_parameters(m)\n",
    "        print(f'Current accuracy: {float(lattice[0][1])}% ({float(lattice[0][1]) - float(inital_acc)}), layers reduced: {reduced} (- {initial_params - current_params} parameters)')\n",
    "    \n",
    "    return lattice[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'named_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mlowranklatticedescenteval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[110], line 11\u001b[0m, in \u001b[0;36mlowranklatticedescenteval\u001b[0;34m(module, tolerance)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlowranklatticedescenteval\u001b[39m(module, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     lowrank_names \u001b[38;5;241m=\u001b[39m \u001b[43mgetcompressablelayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy(module)\n\u001b[1;32m     13\u001b[0m     inital_acc \u001b[38;5;241m=\u001b[39m test(module, val_loader, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[110], line 4\u001b[0m, in \u001b[0;36mgetcompressablelayers\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetcompressablelayers\u001b[39m(module):\n\u001b[1;32m      2\u001b[0m     lowrank_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, ((name, mod), (name_next, mod_next)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mlinearleaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m, linearleaves(module)[\u001b[38;5;241m1\u001b[39m:])):\n\u001b[1;32m      5\u001b[0m         lowrank_names\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lowrank_names\n",
      "Cell \u001b[0;32mIn[69], line 222\u001b[0m, in \u001b[0;36mlinearleaves\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(module, \u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[1;32m    221\u001b[0m linear_children \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_modules\u001b[49m():\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m    224\u001b[0m         linear_children\u001b[38;5;241m.\u001b[39mappend((name, module))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'named_modules'"
     ]
    }
   ],
   "source": [
    "module = lowranklatticedescenteval(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:47,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.8855, Accuracy: 991/10000 (10%)\n",
      "\n",
      "parameters: 22061962 -> 14836718 (- 7225244)\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "inital_params = count_parameters(evaluating)\n",
    "evaluating = svdmodel(evaluating, 190)\n",
    "train(evaluating.to(0), optimizer, train_loader)\n",
    "test(evaluating, val_loader)\n",
    "final_params = count_parameters(evaluating)\n",
    "print(f'parameters: {inital_params} -> {final_params} (- {inital_params - final_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1799, Accuracy: 5494/10000 (55%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [00:49,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1799, Accuracy: 5494/10000 (55%)\n",
      "\n",
      "parameters: 22061962 -> 18779870 (- 3282092)\n"
     ]
    }
   ],
   "source": [
    "model.float().cpu()\n",
    "evaluating = copy(model.eval())\n",
    "inital_params = count_parameters(evaluating)\n",
    "evaluating, neurons = clustermodel(evaluating, 0.1)\n",
    "test(evaluating, val_loader)\n",
    "train(evaluating.to(0), optimizer, train_loader)\n",
    "test(evaluating, val_loader)\n",
    "final_params = count_parameters(evaluating)\n",
    "print(f'parameters: {inital_params} -> {final_params} (- {inital_params - final_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
